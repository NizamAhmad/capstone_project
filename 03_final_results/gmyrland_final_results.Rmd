---
title: 'Vehicle Crash Survivability'
author: "Glen Myrland (500730397)"
date: '`r Sys.Date()`'
output:
    pdf_document:
        fig_caption: yes
---

# Introduction
<!--
    First, provide the context of the problem and then state the problem (your main research question). 
    Second, write briefly that what are you proposing to solve this problem (don't write details of the 
    solution here). (You can use part of your abstract here)
-->

Automotive accidents result in over 30,000 fatalities in the United States annually [[1]](http://www.who.int/violence_injury_prevention/road_safety_status/2015/TableA2.pdf?ua=1).
The National Automotive Sampling System (NASS) provides a nationally representative sample of police reported collisions and is made available to researchers and the general public.
    
The research question for this project is to identify and quantify factors which impact the survivability of various crash types (rear-end, sideswipe, etc) using R, and create a web app using the shiny package to predict survivability for given inputs using regression.
    
The techniques will include web-scraping the publicly available data on the NASS website, parsing the resultant XML and data cleaning of real-world dataset, exploratory analysis to identify relevant factors, feature engineering, and regression.

The source code for this project is available on github at https://github.com/gmyrland/capstone_project.

**References:**

[1] World Health Organization. (2015). Global status report on road safety 2015. Accessed from\newline http://www.who.int/violence_injury_prevention/road_safety_status/2015/TableA2.pdf?ua=1

# Literature Review
<!--
    Write summary of the related papers that you reviewed here.  Write the summary in your own 
    words - don't use the technical jargon from the paper that you don't understand. Keep this section 
    short -  short paragraph or few sentences about each paper you reviewed should be sufficient.
-->

Several publications were reviewed with emphasis being placed on determining potential factors which may have significant effects on vehicle crash survivability.

An Indiana University paper (2014) noted that vehicle inequalities (e.g., height, rigidity, weight) had a significant impact on survivability in head-on collisions.
This driver survival risk factor study found that "the driver's chance of survival was increased by driving a vehicle with a higher mass, driving a newer vehicle, being younger, being a male, using a seatbelt and having the airbag deployed in the crash."&nbsp;[[2]](http://www.eurekalert.org/pub_releases/2014-11/iu-ccs111814.php)

Some studies examined the effect of vehicle age on survivability. 
For example, an Association for the Advancement of Automotive Medicine study (2006) showed decreases in the casualty rate for newer cars in frontal impacts.&nbsp;[[3]](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3217489/)

A 2014 conference paper examined the risk factors associated with the survival of drivers in head on collisions.
In order to control for vehicle speed, vehicles involved in head-on collisions were paired and logistic regression was used to model the effect of other factors such as vehicle mass, vehicle age, and passenger demographics.&nbsp;[[4]](https://www.researchgate.net/publication/266775960_Factors_affecting_survival_in_head-on_vehicle_collisions)

Finally, the World Health Organization report on road traffic injury prevention (2004) identified speed as a key risk factor in road traffic injuries.
Further, driver speed choice was found to be influenced by a number of factors, including:
driver-related factors such as age, gender, alcohol level, and number of people in the vehicle; road and vehicle factors such as road layout, surface quality, vehicle power, and maximum speed; and traffic- and environment-related such as traffic density and composition, prevailing speed, and weather conditions.&nbsp;[[5]](http://www.who.int/violence_injury_prevention/publications/road_traffic/world_report/speed_en.pdf)

**References:**

[2] Indiana University. (2014). Car crash survival rates increase with being younger, male and driving a big vehicle. Accessed from http://www.eurekalert.org/pub_releases/2014-11/iu-ccs111814.php

[3] Frampton, R., Page, M., & Thomas, P. (2006). Factors Related to Fatal Injury in Frontal Crashes Involving European Cars. Annual Proceedings / Association for the Advancement of Automotive Medicine, 50, 35â€“56. 

[4] Kirbiyik, U., Dixon, B., & Zollinger, T.W. (2014). Factors affecting survival in head-on vehicle collisions. 142nd APHA Annual Meeting and Exposition 2014. Accessed from https://www.researchgate.net/publication/266775960_Factors_affecting_survival_in_head-on_vehicle_collisions

[5] World Health Organiztion. (2004). World report on road traffic injury prevention. Accessed from http://www.who.int/violence_injury_prevention/publications/road_traffic/world_report/speed_en.pdf

# Dataset
<!--
    Give the description of the dataset that you are using along with the individual attributes you will or will 
    not use in your analysis. Also mention the source of the dataset (where did you get it from). In case the 
    data is curated and created by you please explain the details. Descriptive statistics of the attributes and 
    datasets can also be provided here.
-->

The data used for this project can be found at http://www.nhtsa.gov/NASS. The section "*NASS CDS Case Viewer - XML Viewer (2004-Present)*" provides a search interface of the existing case data.
When a case id is known, it can be used to extract XML data for the specific collision, allowing for collection of all case data.

>   ``Information collected in NASS, with all personal identifiers removed, is made available to other researchers and organizations involved in the highway safety effort. They include other Federal agencies; state and local governments; universities; research institutions; the automobile, trucking, and insurance industries; and the general public'' ([National Automotive Sampling System, 2008]((http://www.nhtsa.gov/DOT/NHTSA/NCSA/Content/PDF/NASSbrochure.pdf))).

The specific dataset used for analysis in this project is formed by extracting key attributes from the raw XML case data as explained in the approach section below.

<!-- descriptive statistics and attributes used -->
The attributes used include: Crash Configuration, Eyewear, Race, Age, Sex, Airbag Deployment, Posture, Seatbelt status, Entrapment, Alcohol Presence, Roadway Alignment, Posted Speed Limit, Avoidance Maneuver, etc.

Attributes not used include: metadata such as CaseStr (a string case identifier) and paths to image files, EMS data such as type of care administered, detailed vehicle damage information, towing information, accident reconstruction calculated values, detailed restraint information, injuries other than fatalities, and other detailed information beyond the scope of this project.

```{r fig1, echo=FALSE, message=FALSE, fig.height=3.5, fig.width=8, fig.cap="Counts of the various crash configurations."}
library(dplyr)
library(xml2)
library(DBI)
library(RSQLite)
library(knitr)
library(ggplot2)
setwd('~/source/nass')
load('data/df.Rdata')

# character version
dff <- df
factor_cols <- sapply(dff, is.factor)
dff[factor_cols] <- lapply(dff[factor_cols], as.character)

df %>%
    ggplot(aes(x=crash_config)) +
        geom_bar(fill="#0047b3") +
        labs(x = "Configuration", y="Count") +
        theme_bw() +
        theme(axis.text.x = element_text(angle = 30, hjust=1))
```


# Approach
<!--
    Create a block diagram for the steps of your approach to clearly provide an overview. For example, if 
    you first scrapped twitter, second applied NLP techniques to extract keywords, third labelled the tweets 
    as positive and negative using a set of keywords, and fourth build a classifier, then you should create a 
    box for each of the steps with arrows connecting one step to the next one. A sample block diagram is 
    shown below.
    Once this is done, explain each of the steps in detail. What are you planning to do in each step or have 
    already done. For example, in the above case you would create subheadings for each of the steps.  
-->

```{r include=FALSE, eval=FALSE}
library(dplyr)
library(DiagrammeR)
graph <-
  create_graph() %>%
  set_graph_name("Approach") %>%
  set_global_graph_attr("graph", "overlap", "true") %>%
  set_global_graph_attr("graph", "fixedsize", "true") %>%
  set_global_graph_attr("node", "color", "blue") %>%
  set_global_graph_attr("node", "fontname", "Helvetica") %>%
  add_node(label="  1. Collect case ids  ") %>%
  add_node(label="  2. Web-scraping  ", from=1) %>%
  add_node(label="  3. Parse XML  ", from=2) %>%
  add_node(label="  4. Data Cleaning  ", from=3) %>%
  add_node(label="  5. Exploratory Analysis  ", from=4) %>%
  add_node(label="  6. Feature Engineering  ", from=5) %>%
  add_node(label="  7. Regression Analysis  ", from=6) %>%
  select_nodes() %>%  
  set_node_attr_with_selection("shape", "box") %>%
  set_node_attr_with_selection("type", "box") %>%
  clear_selection %>%
  select_edges %>%
  set_edge_attr_with_selection("color", "black") %>%
  clear_selection
render_graph(graph)
## graph produced saved using RStudio as approach.png
```
The approach to be taken is shown in the graph below, and is described in the following subsections.
![graph](approach.png)

## Step 1: Compile complete list of case ids
<!--
    Write details of the step 1. If there is any source code that you'd like to share then provide the link of 
    the Github.
-->

Case data in XML for each collision can be found using a url of the form:\newline http://www-nass.nhtsa.dot.gov/nass/cds/CaseForm.aspx?GetXML&caseid=112007272.

In order to obtain data for each collision, it was necessary to obtain all case ids.
As there was no obvious source for the complete set of ids, and the numerical values of the ids were too sparse for brute-force web scraping, a method was devised to quickly pull all ids from a search results list containing all cases.

The complete set of results can be found using the link http://www-nass.nhtsa.dot.gov/nass/cds/ListForm.aspx and clicking "*Search*".

A Windows application [True X-Mouse Gizmo](http://fy.chalmers.se/~appro/nt/TXMouse/) was used to emulate the Linux behaviour of copying any selected text to the system clipboard.
A macro in the [vim](http://www.vim.org/) text editor was then used to paste the clipboard contents to a text file at a rate of once per second.
Navigating through the result list and selecting all text allowed for quick harvesting of all 49,345 case ids in an unstructured format ([nass_case_ids.txt](https://github.com/gmyrland/capstone_project/blob/master/data/nass_case_ids.txt)).
The result was then filtered for only unique lines containing the regular expression "`[0-9]{9}$`", again using vim.
This provided the original tabular result data in a tidy, tab-delimited file, with the last field being the case id ([nass_case_ids_filtered.txt](https://github.com/gmyrland/capstone_project/blob/master/data/nass_case_ids_filtered.txt)).

## Step 2: Scrape case data using case ids

Using R, the case data was scraped from the NASS website and stored locally as XML. 
Two functions were written to perform the web scrape and are located in [R/scrape.R](https://github.com/gmyrland/capstone_project/blob/master/R/scrape.R).
Given a single case id, `download_case` downloads the case data and saves it as a single text file containing XML with the case id as the name. The function `download_all_cases` uses `download_case` to iteratively download all cases.
If local data already exists for any case, then the case data is not re-downloaded.

## Step 3: Rectangularize key XML fields

Using the [xml2](https://cran.r-project.org/web/packages/xml2/index.html) package in R, many fields in the XML tree were read and stored to a data frame.
Initial attempts using the [XML](https://cran.r-project.org/web/packages/XML/index.html) package found that this package suffered from a severe memory leak bug. Online research revealed no known workarounds, so the code was re-written to use the newer `xml2` package.

Review of the XML structure revealed that there were many fields could be stored on a one-to-one or one-to-many basis with respect to the Case File id.
The `parse_xml` function in [R/parse.R](https://github.com/gmyrland/capstone_project/blob/master/R/parse.R) iterates through the local case data files and produces a single data frame containing the key fields.
Attributes with a one-to-many relationship were stored within nested data frames inside the master dataframe, and were subsequently joined prior to writing to SQLite.
In total, ten SQLite tables were created: Cases, GeneralVehicle, Persons, Events, Safety, Vehicles, EMS, Occupants, Vehicle Exterior, and VehicleInterior.
Each table contains a `CaseId` attribute to identify the original case file.

## Step 4: Form a Single Dataset for Machine Learning

The response variable for the research question is whether or not the occupant of a collision survived.
As such, it makes sense to arrange the data such that each row represents a single unique occupant from the set of all occupants in the sample.
The `build_clean_dataset` function in [R/cleaning.R](https://github.com/gmyrland/capstone_project/blob/master/R/cleaning.R) joins the SQLite tables to form a single dataset by occupant.

This resulted in 105,296 rows containing 704 attributes.
However, the mortality was not known for all occupants, so the dataset was filtered to only include occupants with a mortality of 'Fatal' or 'Not Fatal', leaving 84,277 records.

## Step 5: Data Cleaning

The data collected is real world data and contains missing values.
The missing values were encoded in a number of different ways, such as 'Unknown', 'N/A', 'Not Reported', or ''.
These values were all coerced to `NA` in R.

Furthermore, the data has been collected over time, the XML schema has changed yearly.
Coding and Analytical Manuals for the data are located [here](http://www-nrd.nhtsa.dot.gov/cats/listpublications.aspx?Id=l&ShowBy=DocType), and were referred to while reconciling the data to a consistent schema.

Some attributes were scaled based on unit of measurement attributes.
For example, ages reported in either months or years in the original data were converted to strictly years in the clean dataset.

Attributes with very few values were removed from the dataset.
The response variable was converted to an integer value with 0 representing Non-Fatal and 1 representing Fatal.

Unknown values for text attributes was then converted to the string 'Unknown' as there were considerable `NA` values in the dataset.
The number of unique text values was reduced by combining similar fields, or by coercing very infrequent values to 'Unknown'.
All text values were then coerced to factor variables in R.

Numeric values were scaled, and missing values were imputed using the median value.

## Step 6: Feature Engineering

Several additional attributes were derived from the data, including `is_weekend`, a binary feature indicating whether the crash occurred on the weekend.

## Step 6: Exploratory Analysis

Once the data was cleaned, exploratory analysis was performed.
This included searching for existing correlations in the data, and identifying attributes that would likely be useful in the regression analysis.
The StepAIC function was also used to explore combinations of features that led to improved classification outcomes, and which added little value.

## Step 8: Regression

Finally, a regression analysis was performed to build a model to predict survivability of collisions based on the key inputs identified in the previous steps.
The outcome was validated by performing the same regression on all attributes in the cleaned dataset on Amazon Web Services Machine Learning Platform.

# Results
<!--
    Explain you results here. Consider that you need to communicate your results to executives in an
    organization. For example:
        1. Insert tables and/or charts showing the results
        2. Write description of the table and charts, such that they show the usefulness for an organization
        3. Identify the evaluation measures, such as accuracy, precision, recall, etc.
-->
```{r include=FALSE}
library(ROCR)
```

The data from the cleaned dataset was explored using the StepAIC function in R to determine reasonable candidate features for the model.

## Features Used

The chosen features are summarized in the table below.

```{r, echo=FALSE, result='asis'}
# thes ones work
#terms <- c('crash_config', 'eyewear', 'race', 'age', 'airbag_deployment', 'posture', 'seatbelt_used', 'entrapment', 'event_class', 'damage_plane', 'alcohol_test', 'alcohol_present', 'roadway_alignment', 'posted_speed', 'driver_race', 'compartment_integrity_loss', 'avoidance_maneuver', 'preimpact_location', 'fire', 'drive_Wheels')

terms <- c('compartment_integrity_loss', 'alcohol_test', 'posture', 'seatbelt_used', 'avoidance_maneuver', 'event_class', 'contacted_class', 'alcohol_present', 'contacted_area', 'crash_config', 'preimpact_location', 'contacted', 'entrapment', 'precrash_category', 'preimpact_stability', 'crash_type', 'vehicles_involved', 'alcohol_test_result', 'preevent_movement', 'damage_area')

terms <- c("compartment_integrity_loss", "alcohol_test", "posture", "age", "entrapment",
                "seatbelt_used", "race", "alcohol_present", "avoidance_maneuver", "damage_plane",
                "crash_config", "fire", "rollover_qtr_turns", "posted_speed", "eyewear",
                "airbag_deployment", "seat_orientation", "roadway_alignment", "preimpact_location", "sex")

dff[terms] %>%
    {data.frame(
        Attribute = names(.),
        TypeOf = sapply(., typeof),
        Unique = sapply(., function(x) length(unique(x)))
    )} %>% kable(caption='Class and count of unique values for each attribute.', row.names=FALSE)
```

The values taken by the chosen factor variables used are shown below.

```{r, echo=FALSE, result='asis'}
chr_terms <- terms[terms %in% names(dff[sapply(dff, is.character)])]
dff[chr_terms] %>%
    {data.frame(
        Attribute = names(.),
        Values = sapply(., function(x)
            {t <- paste(unique(x), collapse=", "); ifelse(nchar(t)>70,return(paste(strtrim(t,70), "...")),return(t))}
        )
    )} %>% kable(caption='Class and count of unique values for each attribute.', row.names=FALSE)
```

The relationship between several key features are shown in the following plots.
Mosaic plots are used for categorical attributes, and violin plots are used for numerical attributes.
```{r, echo=FALSE}
ggMMplot <- function(var1, var2){
  # Adapted from http://stackoverflow.com/questions/19233365/how-to-create-a-marimekko-mosaic-plot-in-ggplot2
  levVar1 <- length(levels(var1))
  levVar2 <- length(levels(var2))
  jointTable <- prop.table(table(var1, var2))
  plotData <- as.data.frame(jointTable)
  plotData$marginVar1 <- prop.table(table(var1))
  plotData$var2Height <- plotData$Freq / plotData$marginVar1
  plotData$var1Center <- c(0, cumsum(plotData$marginVar1)[1:levVar1 -1]) + plotData$marginVar1 / 2
  ggplot(plotData, aes(var1Center, var2Height)) +
    geom_bar(stat = "identity", aes(width = marginVar1, fill = var2), col = "Black", size=0.25) +
    geom_text(aes(label = as.character(var1), x = var1Center, y = 0.5, angle=90)) +
      scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0)) +
      xlab("Predictor Proportion") + ylab("Response Proportion") + guides(fill=guide_legend(title="Fatal"))
}
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Age"}
df %>% ggplot(aes(factor(fatal), age)) + geom_violin(fill="darkgreen")
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Sex"}
ggMMplot(df$sex, df$fatal)
```
```{r echo=FALSE, fig.height=2.5, fig.cap="Crash Configuration"}
ggMMplot(df$crash_config, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Seatbelt Used"}
ggMMplot(df$seatbelt_used, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Entrapment"}
ggMMplot(df$entrapment, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Alcohol Present"}
ggMMplot(df$alcohol_present, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Roadway Alignment"}
ggMMplot(df$roadway_alignment, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Avoidance Maneuver"}
ggMMplot(df$avoidance_maneuver, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Compartment Integrity Loss"}
ggMMplot(df$compartment_integrity_loss, df$fatal)
```
```{r, echo=FALSE, fig.height=2.5, fig.cap="Rollover Quarter Turns"}
ggMMplot(df$rollover_qtr_turns, df$fatal)
```

## Partitioning

The data were partitioned into a test and training set using a 70/30 split.
```{r}
set.seed(1234)
n <- nrow(df)
shuffled <- df[sample(n),]
train <- shuffled[1:round(0.7 * n),]
test <- shuffled[(round(0.7 * n) + 1):n,]
```

## Model Fitting

The model was fit using a binomial logistic regression with the `glm` function in R, with `family = binomial` on the training data.
```{r}
f <- reformulate(terms, "fatal")
fit <- glm(f, family = binomial(link="logit"), data=train)
```

## Performance

Probabilities for the response variable based on the test data were assigned using the `predict` function.
```{r}
probs <- predict(fit, test, type="response")
```

This allows us to see the distribution of the predicted response variables.

```{r echo=FALSE, fig.height=4, fig.cap="Histogram of predicted probabilities"}
hist(probs)
```

As would be expected, the model is heavily weighted towards an occupant surviving any given collision.

### Confusion Matrix
Using a cut-off value of 0.5 for the classifier, a confusion matrix of the predicted outcomes can summarize the performance of the model.
```{r, results='asis', echo=FALSE}
conf <- table(test$fatal, as.integer(probs > 0.5))
conf %>% kable(caption="Confusion matrix for the classifier.")
```

Based on the confusion matrix, this classifier appears to perform well for a dataset with this degree of class imbalance.

### Precision, Recall, and Accuracy
```{r, include=FALSE}
TN <- conf[1, 1]; TP <- conf[2, 2]; FP <- conf[1, 2]; FN <- conf[2, 1]
```

Precision, recall, and accuracy are all common measures used to gauge model performance.

Precision is given by `TP/(TP+FP)`.
It is a measure of how likely a fatal outcome predicted by the model actually represents a fatality.
For this model, this gives a value of `r round(TP/(TP+FP),2)`.

Recall is given by `TP/(TP+FN)`.
This is a measure of how likely an actual fatal outcome is correctly predicted by the model.
For this model, this gives a value of `r round(TP/(TP+FN),2)`.

Accuracy is a measure of the proportion of records correctly classified, and is given by `(TP+TN)/(TP+FP+TN+FN)`
For this model, the accuracy is `r round((TP+TN)/(TP+FP+TN+FN),2)`.

The accuracy can be plotted as a function of the chosen threshold value.
The figure below shows that the accuracy is maximized near a threshold value of 0.5, and that there is little variance in the accuracy for threshold values above 0.25.

### Accuracy vs. cut-off threshold
```{r, fig.height=3.5, fig.cap="Effect of varying cut-off threshold on model accuracy."}
acc_at_thresh <- function(threshold) {
    conf <- table(test$fatal, probs > threshold)
    sum(diag(conf))/sum(conf)
}
threshold <- seq(from=0.001, to=1, length=100)
accuracy <- sapply(threshold, acc_at_thresh)
ggplot() + geom_line(aes(x=threshold, y=accuracy))
```

### Chi-Squared
An Chi-Squared analysis of the predictor variables in the model is found below.
```{r}
#anova(fit, test="Chisq")
```

### ROC curve and AUC

A good way to review model performance of a binary classifier is to generate a Receiver Operating Characteristic (ROC) curve.

The ROC curve provides the performance that the model can achieve by varying the cut-off threshold used, and illustrates the trade-off of doing so.
In particular, it shows that if a higher true positive rate (TPR) is required of the model, it will lead to a higher false positive rate (FPR).

```{r, fig.height=4.5, fig.cap="ROC curve for the classifier."}
pred <- prediction(probs, test$fatal)
plot(performance(pred, "tpr", "fpr"))
```

The area under the ROC curve (AUC), is a good general indication of the performance of a model.
An AUC of 0.5 indicates that the model is no better than random chance.
On the other hand, an AUC of 1.0 indicates that the model perfectly explains the response within the test set.

In this case, the AUC is `r round(performance(pred, "auc")@y.values[[1]],3)`.

This is a good AUC for a machine learning model in general, however, the ROC curve is not particularly well suited to data with large class imbalances, since it is not sensitive to the base-rate of the predicted classes.

### Recall-Precision Curve and AUC

Another performance metric, the recall-precision (RP) curve, can be used to examine the model's response.
Similar to the ROC curve, the recall-precision curve shows the effect on model performance as the cut-off threshold varies.
As stated previously, the precision is the how likely a predicted positive outcome is correct, and recall is how likely the model correctly identifies a positive outcome.

```{r, fig.height=4.5, fig.cap="RP curve for the clasifier."}
# Recall-Precision curve             
RP.perf <- performance(pred, "prec", "rec")
plot (RP.perf)
```

Here we can see the effect of the class imbalance.
As the prevalence of the positive case in the data becomes increasingly rare, it becomes more difficult for a model to correctly predict the true positive cases without a significant increase in false positives.
A canonical example of this is the case of cancer screening, where the base-rate can lead to false positives greatly outnumbering true positives, and where the potential harm of a false positive can be high.

The RP curve is sensitive to the response class base-rate, with the effect being that the area under the RP curve tends to decrease with greater imbalance in the response classes.

The area under the RP curve for this model is shown below.
```{r}
RP.perf@y.values[[1]][is.nan(RP.perf@y.values[[1]])] <- 1 # Remove single NaN
caTools::trapz(x=RP.perf@x.values[[1]], y=RP.perf@y.values[[1]])
```

While the area under the ROC curve and the area under the RP curve cannot be directly compared, review of the RP curve and the area under it suggests that the performance of the model is overstated by the area under the ROC curve.

## Performance by Crash Configuration

The following figures show the performance of the generalized classifier for various crash configurations.

```{r, include=FALSE}
f <- reformulate(terms[!terms == "crash_config"], response = "fatal")
fit <- glm(f, family = binomial(link="logit"), data=train)
configurations <- df %>% group_by(crash_config) %>% summarize(count=n()) %>% filter(count > 1000) %>% .$crash_config %>% as.character

dat <- data.frame(type=character(), config=character(), x=double(), y=double())
for (configuration in configurations) {
    ttest <- test[test$crash_config == configuration, ]
    probs <- predict(fit, ttest, type="response")
    pred <- prediction(probs, ttest$fatal)
    perf <- performance(pred, "tpr", "fpr", type="l")
    RP.perf <- performance(pred, "prec", "rec")
    dat <- rbind(
        dat,
        data.frame(type="roc", config=configuration, x=perf@x.values[[1]], y=perf@y.values[[1]]),
        data.frame(type="rp", config=configuration, x=RP.perf@x.values[[1]], y=RP.perf@y.values[[1]])
    )
}
```
```{r figccroc, echo=FALSE, fig.height=3.5, fig.cap="ROC curves for various crash configurations within the test set."}
dat %>%
    filter(type == 'roc') %>%
    ggplot(aes(group=config, colour=config)) + geom_line(aes(x=x, y=y))
```
```{r figccrp, echo=FALSE, warning=FALSE, fig.height=3.5, fig.cap="RP curves for various crash configurations within the test set."}
dat %>%
    filter(type == 'rp') %>%
    ggplot(aes(group=config, colour=config)) + geom_line(aes(x=x, y=y))
```

The classifier appears to do a reasonable job of classifying the survivability of all crash configurations, although classification in crashes involving rollover performs noticeably poorer than the other configurations.

## Amazon Web Services

To validate the findings, Amazon Web Service's cloud based Machine Learning was used to build an additional binary classification model with the entire cleaned dataset, again partitioning a training and test set using a 70/30 split.

The resulting model had an AUC of 0.95, similar to the model developed in R.
The confusion matrix for the model is shown in the table below.

       0   1
-- ----- ---
0  23866 141
1    677 306

The precision was reported as 0.6846, the recall as 0.3133, and the accuracy as 0.9673.
These values are all similar to the values of the model developed in R using only the 20 selected attributes.

# Conclusions
<!--
    Give a short summary (one or two paragraphs) of your analysis and conclude the discussion by defining
    the usefulness of your analysis.
-->
The 
- accuracy, etc
- which features were most imporatant
- comments on consistency with lit review
  -age?, sex?, seatbelt, airbag deployment, vehicle age, speed?, alcohol level, number of people in vehicle
20 attributes out of over 700 available
AUC
RP curve AUC
similarity to AWS

caption all figures and tables
add appropriate headings
make sure spaces are good
add back anova

