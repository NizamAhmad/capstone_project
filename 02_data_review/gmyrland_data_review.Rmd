---
title: 'CKME136 - Data Review for Capstone Project'
author: "Glen Myrland (500730397)"
date: '`r Sys.Date()`'
output: pdf_document
---

# Introduction
<!--
    First, provide the context of the problem and then state the problem (your main research question). 
    Second, write briefly that what are you proposing to solve this problem (don’t write details of the 
    solution here). (You can use part of your abstract here)
-->

Automotive accidents result in over 30,000 fatalities in the United States annually.
The National Automotive Sampling System (NASS) provides a nationally representative sample of police reported collisions and is made available to researchers and the general public.
    
The research question for this project is to identify and quantify factors which impact the survivability of various crash types (Rear-end, Sideswipe, etc) using R, and create a web app using the shiny package to predict survivability for given inputs using regression.
    
The techniques will include web-scraping the publicly available data on the NASS website, parsing the resultant XML and data cleaning of real-world dataset, exploratory analysis to identify relevant factors, feature engineering, and regression.

# Literature Review
<!--
    Write summary of the related papers that you reviewed here.  Write the summary in your own 
    words—don’t use the technical jargon from the paper that you don’t understand. Keep this section 
    short—a short paragraph or few sentences about each paper you reviewed should be sufficient.
-->

# Dataset
<!--
    Give the description of the dataset that you are using along with the individual attributes you will or will 
    not use in your analysis. Also mention the source of the dataset (where did you get it from). In case the 
    data is curated and created by you please explain the details. Descriptive statistics of the attributes and 
    datasets can also be provided here.
-->

The data used for this project can be found at http://www.nhtsa.gov/NASS. The link *NASS CDS Case Viewer - XML Viewer (2004-Present)* provides a web search of the existing case data.
When a case id is known, it can be used to extract XML data for the specific collision, allowing for collection of all case data.
``Information collected in NASS, with all personal identifiers removed, is made available to other researchers and organizations involved in the highway safety effort. They include other Federal agencies; state and local governments; universities; research institutions; the automobile, trucking, and insurance industries; and the general public.'' [[Source]](http://www.nhtsa.gov/DOT/NHTSA/NCSA/Content/PDF/NASSbrochure.pdf)

# Approach
<!--
    Create a block diagram for the steps of your approach to clearly provide an overview. For example, if 
    you first scrapped twitter, second applied NLP techniques to extract keywords, third labelled the tweets 
    as positive and negative using a set of keywords, and fourth build a classifier, then you should create a 
    box for each of the steps with arrows connecting one step to the next one. A sample block diagram is 
    shown below.
    Once this is done, explain each of the steps in detail. What are you planning to do in each step or have 
    already done. For example, in the above case you would create subheadings for each of the steps.  
-->

## Step 1: Compile complete list of Case Ids
<!--
    Write details of the step 1. If there is any source code that you’d like to share then provide the link of 
    the Github.
-->

Case data in XML for each collision can be found using a url of the form:\newline http://www-nass.nhtsa.dot.gov/nass/cds/CaseForm.aspx?GetXML&caseid=112007272.

In order to obtain data for each collision, it was necessary to obtain all Case Ids.
As there was no obvious source for the complete set of ids, and the numerical values of the ids were too sparse for brute force web scraping, a method was devised to quickly pull all ids from the results list of an empty search.

The complete set of results can be found using the link:\newline
http://www-nass.nhtsa.dot.gov/nass/cds/ListForm.aspx

A Windows application [True X-Mouse Gizmo](http://fy.chalmers.se/~appro/nt/TXMouse/) was used to emulate the Linux behaviour of copying any selected text to the system clipboard.
A macro in the [vim](http://www.vim.org/) text editor was then used to paste the clipboard contents to a text file at a rate of once per second.
Navigating through the result list and selecting all text with `Ctrl+A` allowed for quick harvesting of all 49,345 case ids in an unstructured format (found here [nass_case_ids.txt](https://github.com/gmyrland/capstone_project/blob/master/data/nass_case_ids.txt)).
The result was then filtered for only unique lines ending containing the regular expression `[0-9]\{9,9}$`, again using [vim](http://www.vim.org).
The provided the original tabular result data in a tidy, tab-delimited file, with the last field being the case id ([nass_case_ids_filtered.txt](https://github.com/gmyrland/capstone_project/blob/master/data/nass_case_ids_filtered.txt)).

## Step 2: Scrape case data using Case Ids

Using R, the case data can be scraped from the NASS website and stored locally as raw XML. 
Two functions perform the web scrape and are located in [R/scrape.R](https://github.com/gmyrland/capstone_project/blob/master/R/scrape.R).
Given a single case id, `download_case` downloads the case data and saves it as a single text file containing XML with the case id as the name. The function `download_all_cases` uses `download_case` to iteratively download all cases.
If local data already exists for any case, then the case data is not re-downloaded.

## Step 3: Rectangularize key XML fields

Using the [xml2](https://cran.r-project.org/web/packages/xml2/index.html) package in R, key fields in the XML tree can be read and stored to a tidy data frame.
The `parse_xml` function in [R/parse.R](https://github.com/gmyrland/capstone_project/blob/master/R/parse.R) iterates through the local case data files and produces a single dataframe containing the key fields.

## Step 4: Data Cleaning

The data collected is real world data and will contain many missing values.
Furthermore, as it has been collected over time, the XML schema has changed yearly.
Coding and Analytical Manuals for the data are located [here](http://www-nrd.nhtsa.dot.gov/cats/listpublications.aspx?Id=l&ShowBy=DocType), and will be used to reconcile the data to a consistent schema.
Additionally, missing fields may need to be imputed or have corresponding records removed from the data set.

## Step 5: Exploratory Analysis

Once the data is cleaned, exploratory analysis can take place.
This will include searching for existing correlations in the data as well as identification of attributes that will likely be useful in the regression analysis.

## Step 6: Feature Engineering

If required, attributes may need to be refactored or engineered to provide better inputs to the regression.

## Step 7: Regression

Finally, a regression will be performed to build a model to predict survivability of collisions given the inputs identified in the previous steps.
